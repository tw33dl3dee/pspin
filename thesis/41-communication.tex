\chapter{Взаимодействие узлов}
\label{cha:communication}

\section{Выбор средств параллельного выполнения}
\label{sec:parall-selection}

Используемое для параллельного выполнения средство должно обладать следующими свойсвами:
\begin{enumerate}
\item поддерживать модель неразделяемой памяти (системы с разделяемой памятью не
  представляют в данном случае интереса, поскольку основной предпосылкой к
  распараллеливанию является нехватка памяти на отдельной машине);
\item быть достаточно универсальным и распространнеым.
\end{enumerate}

По первому условию подходят три средства~--- MPI (Message Passing Interface), PVM (Parallel
Virtual Machine,~\cite{PVM}) и UPC (Unified Parallel Compiler,~\cite{UPC12}).

MPI является де-факто стандартом написания параллельных приложения для
высокопроизводительных систем и кластеров~\cite{MPI} благодаря поддержке множества языков
(C, C++, Fortran, Java, Python\etc), аппаратных платформ и коммуникационного оборудование
(Ethernet, Infiniband, iWARP\etc). Кроме того, на большинстве кластеров предустановлено
соответствующее ПО для его использования. Исходя из этих преимуществ, в качестве средства
параллельного выполнения был выбран MPI.

\section{Огранизация пересылки состояний}
\label{sec:mpi-interaction}

\paragraph{Сравнение способов и примитивов взаимодействия в MPI}
\label{sec:mpi-primitives}

MPI представляет собой фактически наборов примитивов для обмена сообщениями между
процессами, параллельно выполняющимися на разных узлах с неразделяемой памятью, и для
групповых операций, таких, как синхронизация.

MPI предоставляет следующий набор примитивов взаимодействия.

\begin{enumerate}
\item Синхронная передача сообщений (\Code{Ssend}, \Code{Srecv}). Делающий вызов процесс
  ожидает, пока сообщение не будет доставлено \emph{и} прочитано получателем.
\item Блокирующая передача сообщений (\Code{Send}, \Code{Recv}). Делающий вызов процесс
  ожидает, пока сообщение будет отправлено получателю (при этом его доставка не
  имеет значения).
\item Асинхронная передача сообщений (\Code{Isend}, \Code{Irecv}). Управление возвращается
  процессу немедленно, передача происходит в фоновом режиме.
\item Активный удаленный доступ к памяти (Active RMA, \Code{Win_start},
  \Code{Win_complete}, \Code{Win_post}, \Code{Win_wait}). Каждый процесс создает
  \Term{окно}~--- раздел памяти для всеобщего доступа, при этом один процесс может
  обращаться к окнам других процессов при помощи \Code{Put} и \Code{Get}.
\item Пассивный удаленный доступ к памяти (Passive RMA, \Code{Win_lock},
  \Code{Win_unlock}). Аналогично active RMA, но со принимающего данные процесса не
  требуется какого либо участия.
\end{enumerate}

Синхронная передача очевидно не подходит, поскольку ожидание доставки будет лишней
задержкой в работе, так что из первых трех методов оптимальной является асинхронная
передача.

Наиболее привлекательным выглядит механизм passive RMA, поскольку от принимающего процесса
не требуется каких-либо действий по приему сообщения в очередь, а именно такому поведению
соответствует описанный в~\ref{sec:distr-generation} алгоритм. Однако, RMA, хоть и
является более простым в использовании, на современных реализациях MPI работает медленнее,
чем остальные примитивы~--- это связано с сложностью синхронизации доступа к памяти окнаи
с организацией приема данных на пассивной стороне. На рис.~\ref{fig:mpi-primitives}
показано сравнение их производительности для реализации Intel MPICH.

Для получения этого сравнения была использована утилита mpibench, которая имитирует
передачу сообщений определенного размера между заданным числом узлов в выбранном режиме
(по кольцу, от каждого ко всем\etc).

\begin{figure}[ht]
  \centering
  %\includegraphics[width=0.5\textwidth]{../graphics/mpi-primitives}
  \caption{Сравнение производительности примитивов MPI}
  \label{fig:mpi-primitives}
\end{figure}

Из сравнения видно, что RMA почти в 4 раза медленнее, чем асинхронная передача
сообщений. Поскольку ожидается, что задержки на передачу сообщений будут составлять
большую часть времени генерации, в качестве используемого примитива была выбрана
асинхронная передача-прием сообщений, несмотря на удобство RMA.

\paragraph{Схема асинхронного обмена сообщениями}
\label{sec:async-mpi-queue}

После того, как буфер с данными отправлен вызовом \Code{Isend}, его нельзя вторично
использовать, пока сообщение не будет доставлено~--- необходимо дождаться завершения
операции доставки. Поэтому используется набор из $Q_1$ предвыделенных буферов (оптимальное
значение $Q_1$ необходимо выбирать, исходя из параметров кластера, скорости работы
сети\etc).

Отправка сообщения происходит следующим образом:
\begin{enumerate}
\item выбирается первый свободный буфер (не помеченный как участвующий в асинхронной
  операции);
\item если такой буфер не найден~--- делается вызов \Code{Waitany} по всему набору
  буферов, который возвращает управление, как только хотя бы одна из операций завершится;
\item операции по некоторым буферам уже могли завершиться к этому моменту~--- в этом
  случае \Code{Waitany} возвращает управление немедленно, помечая первый из таких буферов
  как свободный; при этом может больше одной завершившейся операции, и, чтобы избежать
  повторных вызовов \Code{Waitany} при посылке следующих сообщений, \Code{Testany}
  вызывается в цикле, пока все такие буфера не будут помечены освобожденными;
\item передавемое состояние копируется в буфер;
\item делается вызов \Code{Isend}, запускающий асинхронную передачу, и буфер помечается
  используемым.
\end{enumerate}

Для приема сообщений используется другой набор из $Q_2$ буферов (для простоты используется
одно и то же $Q_1 = Q_2 = Q$). Каждый из них предварительно передается в вызов
\Code{Irecv}, таким образом, запускается сразу $Q_2$ операций асинхронного приема. Прием
сообщения делается следующим образом:
\begin{enumerate}
\item делается вызов \Code{Waitany}, возвращающий управление, когда хотя бы одна из
  операций приема завершится;
\item по аналогии с приемом, \Code{Waitany} может вернуть управление сразу;
\item производится работа с принятым буфером;
\item когда буфер больше не нужен, делается вызов \Code{Irecv}, запускающий очередную
  операцию приема.
\end{enumerate}

!!! тут можно блок-схему. или нахрен

Возможен также вариант приема сообщения без ожидания, когда вместо \Code{Waitany}
используется \Code{Testany}, проверяющий, нет ли уже завершенных операций
приема. Подробнее об использовании обоих вариантов говорится далее
в~\ref{sec:local-network-queue}.

Наборы буферов приема-отправки выделены в отдельные высокоуровневые примитивы (асинхронные
очереди MPI) с отдельными функциями для работы с ними.

Описанная схема асинхронного взаимодействия показана в виде диаграммы последовательностей
на рис.~\ref{fig:mpi-async-seq}. Здесь \Code{P1} и \Code{P2}~--- два взаимодействующих
процесса на разных узлах, один показан в роли отправителя, другой~--- в роли
приемника. \Code{C1} и \Code{C2}~--- их MPI-коммуникаторы, а \Code{Q1} и \Code{Q2}
представляют собой описанные выше асинхронные очереди.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.1\textwidth]{../graphics/mpi-async-seq}  
  \caption{Асинхронное взаимодействие узлов (MPI)}
  \label{fig:mpi-async-seq}
\end{figure}

\section{Локальная и сетевая очереди состояний}
\label{sec:local-network-queue}

В описании приема сообщений в~\ref{sec:async-mpi-queue} не сказано, в какие именно моменты
происходит прием MPI-сообщений. В алгоритме, приведенном в~\ref{sec:distr-generation},
есть операция выборки следующего состояния из очереди, \Code{Queue $\rightarrow$ state}.

В реальности, у какждого узла фактически есть две очереди: локальная очередь
\Code{Queue$_L$}, куда делается вставка новых локально сгенерированных состояний, и
неявная сетевая очередь \Code{Queue$_N$}, представленная набором буферов
из~\ref{sec:async-mpi-queue} (неявная потому, что какой-либо порядок добавления--выборки в
ней отсутствует). 

При отправке состояния другому узлу оно вставляется в его сетевую очередь, а при выборке
надо использовать обе очереди. Можно предложить два варианта опроса сетевой очереди:
\begin{enumerate}
\item всегда сначала выбирать состояние из локальной очереди; если локальная очередь
  закончилась, ожидать появления состояний в сетевой и выбирать из нее;
\item проверять сначала сетевую очередь на предмет наличия в ней сообщения; если их нет,
  проверять локальную; если обе пусты, ожидать появления сообщений в сетевой.
\end{enumerate}

Оба метода имеют свои достоинства и недостатки.
\begin{itemize}
\item При небольшом количестве узлов локальная очередь состояний на отдельном узле может
  довольно долго не опустевать~--- если каждое состояние порождает несколько новых, и хотя
  бы одно из них принадлежит текущему узлу. Тогда, если сначала опрашивается локальная
  очередь, сообщения подолгу не будут приниматься этим узлом, а у других узлов, по мере
  посылки ему сообщений, будут скапливаться незавершенные операции.
\item Прием сообщений сначала из \Code{Queue$_N$} может приводить к большому росту
  \Code{Queue$_L$}. При полном хэшировании (см.~\ref{sec:fullhash-store}) размер
  \Code{Queue$_L$} не играет значения: \Code{Queue$_L$} и \Code{Visited} хранятся в одной
  области памяти, поэтому порядок добавления в \Code{Queue$_L$} ничего не меняет: все
  добавленные в нее состояния рано или поздно будут перенесены в \Code{Visited}. Однако
  при битовом хэшировании (см.~\ref{sec:bithash-store}) состояния не хранятся нигде вне
  обоих очередей, и под \Code{Queue$_L$} отводится отдельная область памяти, увеличение
  размера которой приводит к уменьшению размера хэш-таблицы.
\end{itemize}

Для того, чтобы минимизировать сетевые задержки, предпочтение отдается второму варианту:
сначала проверяется сетевая очередь. Псевдокод алгоритма, с приведенными уточнениями,
имеет вид:

\begin{lstlisting}[style=pseudocode]
Visited = ()
QueueL  = ()
QueueN  = ()

def NextState():
    if not empty(QueueN):
        QueueN -> NextState
    elif not empty(QueueL):
        QueueL -> NextState
    else:
        wait(QueueN)
        QueueN -> NextState

def ParStateSpaceBFS():
    state = initial_state
    do:
        node = StateNode(state)
        if NodeId = node:
            if not state in Visited:
                Visited <- state
                for each new_state in Next(state):
                    QueueL <- new_state
        else:
            node.QueueN <- state
        state = NextState()

ParStateSpaceBFS()
\end{lstlisting}

Возможны две схемы использования состояния, взятых из сетевой очереди \Code{Queue$_N$}:
скопировать их в отдельную область памяти (например, в \Code{Queue$_L$}), после чего сразу
отдать буфер обратно в асинхронную очередь, либо использовать состояние прямо в буфере и
отдать его после завершение обработки состояния. 

При полном хэшировании состояний вторая схема неприменима: в этом случае в хэш-таблицу
будет добавлен указатель на состояние в MPI-буфере, а не в \Code{Visited}, куда состояние
будет перенесено после обработки. Поэтому при полном хэшировании всегда используется
первая схема, тогда как при битовом можно выбрать любую из двух (одна обеспечивает меньший
размер \Code{Queue$_L$}, другая~--- меньшие сетевые задержкие).

\section{Распределенное завершение}
\label{sec:distributed-termination}

\paragraph{Алгоритм Дейкстры}
\label{sec:distr-term-dijkstra}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-problem}  
  \caption{Задача распределенного завершения}
\label{fig:termination-problem}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-naive}  
  \caption{Наивное решение}
\label{fig:termination-naive}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-dijkstra}  
  \caption{Алгоритм Дейкстры}
\label{fig:termination-dijkstra}
\end{figure}

\section{Журналирование и отладка}
\label{sec:mpi-logging}


\section{Формат сообщений}
\label{sec:message-format}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 

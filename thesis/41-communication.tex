\chapter{Взаимодействие узлов}
\label{cha:communication}

\section{Выбор средств параллельного выполнения}
\label{sec:parall-selection}

Используемое для параллельного выполнения средство должно обладать следующими свойсвами:
\begin{enumerate}
\item поддерживать модель неразделяемой памяти (системы с разделяемой памятью не
  представляют в данном случае интереса, поскольку основной предпосылкой к
  распараллеливанию является нехватка памяти на отдельной машине);
\item быть достаточно универсальным и распространнеым.
\end{enumerate}

По первому условию подходят три средства -- MPI (Message Passing Interface), PVM (Parallel
Virtual Machine,~\cite{PVM}) и UPC (Unified Parallel Compiler,~\cite{UPC12}).

MPI является де-факто стандартом написания параллельных приложения для
высокопроизводительных систем и кластеров~\cite{MPI} благодаря поддержке множества языков
(C, C++, Fortran, Java, Python\etc), аппаратных платформ и коммуникационного оборудование
(Ethernet, Infiniband, iWARP\etc). Кроме того, на большинстве кластеров предустановлено
соответствующее ПО для его использования. Исходя из этих преимуществ, в качестве средства
параллельного выполнения был выбран MPI.

\section{Огранизация пересылки состояний}
\label{sec:mpi-interaction}

\subsection{Сравнение способов и примитивов взаимодействия в MPI}
\label{sec:mpi-primitives}

MPI представляет собой фактически наборов примитивов для обмена сообщениями между
процессами, параллельно выполняющимися на разных узлах с неразделяемой памятью, и для
групповых операций, таких, как синхронизация.

MPI предоставляет следующий набор примитивов взаимодействия.

\begin{enumerate}
\item Синхронная передача сообщений (\Code{Ssend}, \Code{Srecv}). Делающий вызов процесс
  ожидает, пока сообщение не будет доставлено \emph{и} прочитано получателем.
\item Блокирующая передача сообщений (\Code{Send}, \Code{Recv}). Делающий вызов процесс
  ожидает, пока сообщение будет отправлено получателю (при этом его доставка не
  имеет значения).
\item Асинхронная передача сообщений (\Code{Isend}, \Code{Irecv}). Управление возвращается
  процессу немедленно, передача происходит в фоновом режиме.
\item Активный удаленный доступ к памяти (Active RMA, \Code{Win_start},
  \Code{Win_complete}, \Code{Win_post}, \Code{Win_wait}). Каждый процесс создает
  \Term{окно} -- раздел памяти для всеобщего доступа, при этом один процесс может
  обращаться к окнам других процессов при помощи \Code{Put} и \Code{Get}.
\item Пассивный удаленный доступ к памяти (Passive RMA, \Code{Win_lock},
  \Code{Win_unlock}). Аналогично active RMA, но со принимающего данные процесса не
  требуется какого либо участия.
\end{enumerate}

!!! Сравнение и выбор.

\subsection{Схема асинхронного обмена сообщениями}
\label{sec:async-mpi-queue}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.1\textwidth]{../graphics/mpi-async-seq}  
  \caption{Асинхронное взаимодействие узлов (MPI)}
  \label{fig:mpi-async-seq}
\end{figure}

\subsection{Формат сообщений}
\label{sec:message-format}

\section{Локальная и сетевая очереди состояний}
\label{sec:local-network-queue}

\section{Распределенное завершение}
\label{sec:distributed-termination}

\subsection{Алгоритм Дейкстры}
\label{sec:distr-term-dijkstra}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-problem}  
  \caption{Задача распределенного завершения}
\label{fig:termination-problem}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-naive}  
  \caption{Наивное решение}
\label{fig:termination-naive}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-dijkstra}  
  \caption{Алгоритм Дейкстры}
\label{fig:termination-dijkstra}
\end{figure}

\section{Журналирование и отладка}
\label{sec:mpi-logging}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 

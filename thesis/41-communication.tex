\chapter{Программная реализация параллельной генерации состояний}
\label{cha:communication}

\section{Выбор платформы параллельных вычислений}
\label{sec:parall-selection}

Используемая платформа параллельных выполнений должна обладать следующими свойсвами:
\begin{enumerate}
\item поддерживать модель неразделяемой памяти (системы с разделяемой памятью не
  представляют в данном случае интереса, поскольку основной предпосылкой к
  распараллеливанию является нехватка памяти на отдельной машине);
\item быть достаточно универсальной и распространной.
\end{enumerate}

По первому условию подходят три средства~--- MPI (Message Passing Interface), PVM (Parallel
Virtual Machine,~\cite{PVM}) и UPC (Unified Parallel~C,~\cite{UPC12}).

MPI является де-факто стандартом написания параллельных приложения для
высокопроизводительных систем и кластеров~\cite{MPI} благодаря поддержке множества языков
(C, C++, Fortran, Java, Python\etc), аппаратных платформ и коммуникационного оборудование
(Ethernet, Infiniband, iWARP\etc). Кроме того, на большинстве кластеров предустановлено
соответствующее ПО для его использования. Исходя из этих преимуществ, в качестве
используемой платформы выбран стандарт MPI.

\section{Огранизация пересылки состояний}
\label{sec:mpi-interaction}

\paragraph{Сравнение способов и примитивов взаимодействия в MPI}

MPI представляет собой фактически наборов примитивов для обмена сообщениями между
процессами, параллельно выполняющимися на разных узлах с неразделяемой памятью, и для
групповых операций, таких, как синхронизация.

MPI предоставляет следующий набор примитивов взаимодействия.

\begin{enumerate}
\item Синхронная передача сообщений. Делающий вызов процесс ожидает, пока сообщение не
  будет доставлено \emph{и} прочитано получателем.
\item Блокирующая передача сообщений. Делающий вызов процесс ожидает, пока сообщение будет
  отправлено получателю (при этом его доставка не имеет значения).
\item Асинхронная передача сообщений. Управление возвращается процессу немедленно,
  передача происходит в фоновом режиме.
\item Активный удаленный доступ к памяти (Active RMA). Каждый процесс создает
  \Term{окно}~--- раздел памяти для всеобщего доступа, при этом один процесс может
  обращаться к окнам других процессов при помощи специальных вызовов.
\item Пассивный удаленный доступ к памяти (Passive RMA). Аналогично active RMA, но со
  принимающего данные процесса не требуется какого либо участия.
\end{enumerate}

Синхронная передача очевидно не подходит, поскольку ожидание доставки будет лишней
задержкой в работе, так что из первых трех методов оптимальной является асинхронная
передача.

Наиболее привлекательным выглядит механизм passive RMA, поскольку от принимающего процесса
не требуется каких-либо действий по приему сообщения в очередь, а именно такому поведению
соответствует описанный в разделе~\ref{sec:distr-generation} алгоритм. Однако, RMA, хоть и
является более простым в использовании, на современных реализациях MPI работает медленнее,
чем остальные примитивы~--- это связано с сложностью синхронизации доступа к памяти окнаи
с организацией приема данных на пассивной стороне. На рис.~\ref{fig:mpi-primitives}
показано сравнение их производительности для реализации Intel MPICH.

Для получения этого сравнения была использована утилита \Code{mpibench}, которая имитирует
передачу сообщений определенного размера между заданным числом узлов в выбранном режиме
(по кольцу, от каждого ко всем\etc).

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.05\textwidth]{../data/plots/mpi}
  \caption{Сравнение производительности примитивов MPI}
  \label{fig:mpi-primitives}
\end{figure}

Из сравнения видно, что passive RMA почти на порядок медленнее асинхронной передачи
сообщений, а active RMA~--- примерно в 5 раз. Поскольку ожидается, что задержки на
передачу сообщений будут составлять существенную часть времени генерации, в качестве
используемого примитива была выбрана асинхронная передача-прием сообщений, несмотря на
удобство RMA.

\paragraph{Схема асинхронного обмена сообщениями}

После того, как буфер с данными отправлен вызовом \Code{Isend}, его нельзя вторично
использовать, пока сообщение не будет доставлено~--- необходимо дождаться завершения
операции доставки. Поэтому используется набор из $Q_1$ предвыделенных буферов (оптимальное
значение $Q_1$ необходимо выбирать, исходя из параметров кластера, скорости работы
сети\etc).

Отправка сообщения происходит следующим образом.
\begin{enumerate}
\item Выбирается первый свободный буфер (не помеченный как участвующий в асинхронной
  операции).
\item Усли такой буфер не найден~--- делается вызов \Code{Waitany} по всему набору
  буферов, который возвращает управление, как только хотя бы одна из операций завершится;
\item Операции по некоторым буферам уже могли завершиться к этому моменту~--- в этом
  случае \Code{Waitany} возвращает управление немедленно, помечая первый из таких буферов
  как свободный; при этом может больше одной завершившейся операции, и, чтобы избежать
  повторных вызовов \Code{Waitany} при посылке следующих сообщений, \Code{Testany}
  вызывается в цикле, пока все такие буфера не будут помечены освобожденными.
\item Передавемое состояние копируется в буфер.
\item Делается вызов \Code{Isend}, запускающий асинхронную передачу, и буфер помечается
  используемым.
\end{enumerate}

Описанный алгоритм отправки изображен в виде блок-схемы на
рис.~\ref{fig:mpi-send-flowchart}.

\begin{figure}[!tb]
  \centering
  \includegraphics[height=0.8\textheight]{../graphics/mpi-send-flowchart}
  \caption{Блок-схема алгоритма отправки сообщения}
  \label{fig:mpi-send-flowchart}
\end{figure}

Для приема сообщений используется другой набор из $Q_2$ буферов (для простоты используется
одно и то же $Q_1 = Q_2 = Q$). Каждый из них предварительно передается в вызов
\Code{Irecv}, таким образом, запускается сразу $Q_2$ операций асинхронного приема. Прием
сообщения делается следующим образом:
\begin{enumerate}
\item делается вызов \Code{Waitany}, возвращающий управление, когда хотя бы одна из
  операций приема завершится;
\item по аналогии с приемом, \Code{Waitany} может вернуть управление сразу;
\item производится работа с принятым буфером;
\item когда буфер больше не нужен, делается вызов \Code{Irecv}, запускающий очередную
  операцию приема.
\end{enumerate}

Возможен также вариант приема сообщения без ожидания, когда вместо \Code{Waitany}
используется \Code{Testany}, проверяющий, нет ли уже завершенных операций
приема. Подробнее об использовании обоих вариантов говорится далее в
разделе~\ref{sec:local-network-queue}.

Наборы буферов приема-отправки выделены в отдельные высокоуровневые примитивы (асинхронные
очереди MPI) с отдельными функциями для работы с ними.

Описанная схема асинхронного взаимодействия двух процессов показана в виде диаграммы
последовательностей на рис.~\ref{fig:mpi-async-seq}. Здесь \Code{P1} и \Code{P2}~--- два
взаимодействующих процесса на разных узлах, один показан в роли отправителя, другой~--- в
роли приемника. \Code{C1} и \Code{C2}~--- их MPI-коммуникаторы, а \Code{Q1} и \Code{Q2}
представляют собой описанные выше асинхронные очереди.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1.1\textwidth]{../graphics/mpi-async-seq}  
  \caption{Асинхронное взаимодействие узлов (MPI)}
  \label{fig:mpi-async-seq}
\end{figure}

% \begin{figure}[ht]
%   \centering
%   \small
%   \begin{sequencediagram}
%     \newthread{p1}{P1:процесс}
%     \newinst{q1}{Q1:очередь}
%     \newinst{c1}{C1:комм.}
%     \newinst[1]{c2}{C2:комм.}
%     \newinst{q2}{Q2:очередь}
%     \newthread[red]{p2}{P2:процесс}

%     \begin{call}{p1}{получить буфер}{q1}{буфер}
%       \prelevel
%       \begin{call}{p2}{освободить буфер}{q2}
%         \prelevel
%         \mess{q2}{MPI\_Irecv}{c2}
%       \end{call}
%     \end{call}

%     \begin{callself}{p2}{локальная работа}{}
%       \prelevel
%       \begin{callself}{p1}{запись данных в буфер}{}
%       \end{callself}
%       \begin{call}{p1}{отправка буфера}{q1}
%         \prelevel
%         \mess{q1}{MPI\_Isend}{c1}
%         \mess{c1}{данные}{c2}
%       \end{call}
%     \end{callself}
    
%   \end{sequencediagram}
%   \caption{Асинхронное взаимодействие узлов (MPI)}
%   \label{fig:mpi-async-seq}
% \end{figure}

\section{Локальная и сетевая очереди состояний}
\label{sec:local-network-queue}

В описании приема сообщений в разделе~\ref{sec:mpi-interaction} не сказано, в какие именно
моменты происходит прием MPI-сообщений. В алгоритме, приведенном в
разделе~\ref{sec:distr-generation}, есть операция выборки следующего состояния из очереди,
\Code{Queue $\rightarrow$ state}.

В реальности, у какждого узла фактически есть две очереди: локальная очередь
\Code{Queue$_L$}, куда делается вставка новых локально сгенерированных состояний, и
неявная сетевая очередь \Code{Queue$_N$}, представленная набором MPI-буферов (неявная
потому, что какой-либо порядок добавления--выборки в ней отсутствует).

При отправке состояния другому узлу оно вставляется в его сетевую очередь, а при выборке
надо использовать обе очереди. Можно предложить два варианта опроса сетевой очереди:
\begin{enumerate}
\item всегда сначала выбирать состояние из локальной очереди; если локальная очередь
  закончилась, ожидать появления состояний в сетевой и выбирать из нее;
\item проверять сначала сетевую очередь на предмет наличия в ней сообщения; если их нет,
  проверять локальную; если обе пусты, ожидать появления сообщений в сетевой.
\end{enumerate}

Оба метода имеют свои достоинства и недостатки. При небольшом количестве узлов локальная
очередь состояний на отдельном узле может довольно долго не опустевать~--- если каждое
состояние порождает несколько новых, и хотя бы одно из них принадлежит текущему
узлу. Тогда, если сначала опрашивается локальная очередь, сообщения подолгу не будут
приниматься этим узлом, а у других узлов, по мере посылки ему сообщений, будут
скапливаться незавершенные операции.

Прием сообщений сначала из \Code{Queue$_N$} может приводить к большому росту
\Code{Queue$_L$}. При полном хэшировании (см.~раздел~\ref{sec:fullhash-store}) размер
\Code{Queue$_L$} не играет значения: \Code{Queue$_L$} и \Code{Visited} хранятся в одной
области памяти, поэтому порядок добавления в \Code{Queue$_L$} ничего не меняет: все
добавленные в нее состояния рано или поздно будут перенесены в \Code{Visited}. Однако при
битовом хэшировании (см.~раздел~\ref{sec:bithash-store}) состояния не хранятся нигде вне
обоих очередей, и под \Code{Queue$_L$} отводится отдельная область памяти, увеличение
размера которой приводит к уменьшению размера хэш-таблицы.

Для того, чтобы минимизировать сетевые задержки, предпочтение отдается второму варианту:
сначала проверяется сетевая очередь. Псевдокод алгоритма, с приведенными уточнениями,
имеет вид:

\begin{lstlisting}[style=pseudocode]
Visited = ()
QueueL  = ()
QueueN  = ()

def NextState():
    if not empty(QueueN):
        QueueN -> NextState
    elif not empty(QueueL):
        QueueL -> NextState
    else:
        wait(QueueN)
        QueueN -> NextState

def ParStateSpaceBFS():
    state = initial_state
    do:
        node = StateNode(state)
        if NodeId = node:
            if not state in Visited:
                Visited <- state
                for each new_state in Next(state):
                    QueueL <- new_state
        else:
            node.QueueN <- state
        state = NextState()

ParStateSpaceBFS()
\end{lstlisting}

Возможны две схемы использования состояния, взятых из сетевой очереди \Code{Queue$_N$}:
скопировать их в отдельную область памяти (например, в \Code{Queue$_L$}), после чего сразу
отдать буфер обратно в асинхронную очередь, либо использовать состояние прямо в буфере и
отдать его после завершение обработки состояния. 

При полном хэшировании состояний вторая схема неприменима: в этом случае в хэш-таблицу
будет добавлен указатель на состояние в MPI-буфере, а не в \Code{Visited}, куда состояние
будет перенесено после обработки. Поэтому при полном хэшировании всегда используется
первая схема, тогда как при битовом можно выбрать любую из двух (одна обеспечивает меньший
размер \Code{Queue$_L$}, другая~--- меньшие сетевые задержкие).

\section{Распределенное завершение работы}
\label{sec:distributed-termination}

Приведенный выше алгоритм не содержит в себе никакого условия завершения. Задача
обнаружения завершения в параллельных расчетах без разделяемой памяти является
нетривиальной: в условиях отсутствия средств атомарного опроса всех остальных узлов
необходим алгоритм, который бы позволил хотя бы одному узлу определить, что вычисления
завершились на всех узлах \emph{и} никаких отправленных, но еще не принятых сообщений в
системе нет.

На рис.~\ref{fig:termination-problem} показан пример взаимодействия узлов, развернутого во
времени. Жирные линии означают, что узел занимается обработкой, стрелки~--- передаваемые
сообщения. Из него видно, что простого опроса узлов, заняты ли они вычислениями,
недостаточно: в момент времени $T_1$ все узлы простаивают, однако имеется непринятое
сообщение, отправленное узлом 3. Настоящим моментом завершения является момент $T_2$,
когда все узлы простаивают \emph{и} все сообщения достигли назначения.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-problem}  
  \caption{Демонстрация задачи о распределенном завершении}
\label{fig:termination-problem}
\end{figure}

\paragraph{Наивное решение}

Наивное решение задачи заключается в том, чтобы ввести на каждом узле $i$ счетчик $C_i$
сообщений, находящихся в пути. Изначально во всех узлах этот счетчик равен $0$, при
отправке сообщения другому узлу он увеличивается на $1$, при приеме~--- уменьшается на
$1$. Сумма счетчиков всех узлов $\sum_iC_i$ в выбранный момент времени есть число
находящихся в пути сообщений. Для подсчета этой суммы на каждом узле вводится
дополнительный счетчик-аккумулятор $A_i$. Первый узел инициализирует его значением $C_i$ и
передает первому узлу. Когда первый узел завершает всю локальную обработку и входит в
состояние простое, он прибавляет к аккумулятору $C_1$ и передает второму\etc. Последний
узел, завершив работу, передает обратно первому $A_N = \sum_iC_i$ и, если это значение
равно нулю, первый узел делает вывод, что сообщений в пути нет и оповещает всех остальных
о завершении. В противном случае ($A_N \neq 0$), первый узел отправляет второму новый
счетчик-аккумулятор, и процесс повторяется заново.

Такой <<наивный>> подход неправильно работает при сценарии, показанном на
рис.~\ref{fig:termination-naive}. Контрольные сообщения, содержащие счетчик-аккумулятор,
показаны курсивом, рядом с ними показано значение этого счетчика. Поскольку узел 3
отправил состояние, которое еще не было получено узлом 2 в момент <<прохождения>> через
него аккумулятора, а узел 4 успел принять сообщение от узла 2 до приема аккумулятора,
суммарное число сообщений выходит нулевым. Сообщения, посланные узлу 2 от узла 3 и узлу 4
от узла 2, не вошли в этот аккумулятор, в результате чего завершение будет обнаружено
узлом 1 неверно~--- узел 2, получивший новое сообщение уже после отправки аккумулятора,
может продолжать его обработку в момент объявления завершения.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-naive}  
  \caption{Наивное решение}
\label{fig:termination-naive}
\end{figure}

\paragraph{Алгоритм Дейкстры распределенного завершения}

Для решения возникающей проблемы предлагается алгоритм, впервые описанный
Дейкстрой~\cite{DistrTerm}. Идея его заключается в том, что каждому узлу приписывается
цвет (в оригинале~--- белый и черный, здесь мы будем называть их красный и синий). Цвет
также приписывается аккумулятору, передаваемому между узлами.

Цвет определяется следующими правилами:
\begin{enumerate}
\item когда узел принимает новое сообщение и начинает его обработку~--- он становится красным;
\item когда узел отправляет аккумулятор следующему узлу~--- он становится синим;
\item первый узел отправляет синий аккумулятор;
\item когда красный узел отправляет аккумулятор, последний также становится красным.
\end{enumerate}

Распределенное завершение обнаруживается первым узлом, если принятый от последнего узла
аккумулятор~--- синий. Это означает, что ни один узел не занимался обработкой на
протяжении участка времени от предпоследней до последней отправки аккумулятора и,
следовательно, во время последнего <<витка>> аккумулятора ни один узел не посылал новых
сообщений. Наглядно работа этого алгоритма для того же сценария продемонстрирована на
рис.~\ref{fig:termination-dijkstra}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=1\textwidth]{../graphics/termination-dijkstra}  
  \caption{Алгоритм Дейкстры}
\label{fig:termination-dijkstra}
\end{figure}

\section{Журналирование работы системы}
\label{sec:mpi-logging}

Для отладки параллельной программы необходима возможность отладочного журналирования.
Обычно на кластере у всех узлов есть общий доступ к дисковому пространству через сетевую
файловую систему (NFS или GPFS), поэтому можно выводить отладочные сообщения в файл на
диске.

Если каждый узел будет осуществлять вывод в журнал самостоятельно, высока вероятность
<<перекрытия>> строк от различных узлов. Можно использовать средства MPI-IO~\cite{MPI},
предоставляющие средства синхронизации доступа к файловой системе, однако, поскольку при
отладке высокая производительность не требуется, используется другое, более простой,
способ: при запуске в отладочной конфигурации один из узлов используется как служба
журналирования. Остальные узлы вместо непосредственного вывода отладочных сообщений в файл
посылают их в виде сообщений службе журналирования, который находится в постоянном цикле
приема таких сообщений и выводит их в журнал.

\section{Формат сообщений}
\label{sec:message-format}

Каждое сообщение в MPI, помимо данных, содержит тег (MPI tag), который может
использоваться, как идентификатор типа сообщения. В частности, возможен прием лишь
сообщений с конкретным тегом, однако данная функция используется лишь службой
журналирования (который не принимает никаких сообщений, кроме отладочного вывода).

Всего используется 4 типа сообщений, каждое имееющее свой тег.
\begin{itemize}
\item Сообщение с новым состоянием. Содержит состояние в виде массива байт
  (\Code{MPI\_CHAR}). Представление самого состояния описано далее в
  разделе~\ref{sec:state-represent}.
\item Сообщение с счетчиком-аккумулятором (для обнаружения завершения,
  см.~раздел~\ref{sec:distributed-termination}). Содержит 2 элемента \Code{MPI\_INT}:
  счетчик и его цвет (0~--- синий, 1~--- красный).
\item Сообщение с объявлением о завершении. Рассылается всем остальным узлам, когда
  какой-то узел обнаруживает завершение (это может быть первый узел, если завершение
  обнаружено алгоритмом Дейкстры, или любой другой, если завершение происходит из-за
  нахождения контпримера $\pi_e$). Состоит из одного элемента \Code{MPI\_INT}~--- номера
  узла, обнаружившего завершение.
\item Сообщение с отладочным выводом. Должно посылаться лишь узлу со службой
  журналирования и обрабатывается лишь им. Содержит строку в виде символьного массива
  (\Code{MPI\_CHAR}).
\end{itemize}

\section{Тестирование}
\label{sec:stategen-testing}

Проводилось функциональное и регресионное тестирование генератора состояний.

\paragraph{Функциональное тестирование}

Выделяемые классы эквивалентности показаны на табл.~\ref{tab:stategen-systests-equiv}.

\begin{table}[ht]
  \centering
  \caption{Классы эквивалентности входных данных}
  \begin{tabular}{|p{0.5\textwidth}|p{0.5\textwidth}|}
    \hline
    Входные данные & Выходные данные \\
    \hline
    Модель, содержащая взаимоблокировку & Контрпример в виде состояния блокировки \\ \hline
    Модель, содержащая нарушения утверждений о значениях переменных & Контрпример в виде
    состояния, в котором нарушаются утверждения \\ \hline
    Модель, соответствующая заданным утверждениям & Сообщение о выполнениии утверждений на
    всем пространстве состояний \\
    \hline
  \end{tabular}
  \label{tab:stategen-systests-equiv}
\end{table}

\paragraph{Регресионное тестирование}

В ходе регресионного тестирования проверялось утверждение, что множество генерируемых
состояний не изменяется после внесения исправлений в программу (т.е., что не внесены
ошибки в процесс генерации состояний).

При выполнении регресионного тестирования набор хэш-кодов всех сгенерированных состояний
для тестовой модели сохраняется в журнал и проводится сравнение c тем же набором,
полученным до внесения изменений.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
